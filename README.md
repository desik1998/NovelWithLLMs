This project showcases the ability to write a long story/novel **well grounded in details, feels human like and has coherence of events** using LLM. Although the story is heavily grounded in details, this project doesn't aim to create a **SOTA story** but is just a demonstration of how to generate long stories well grounded in details using LLMs

Currently a story of 35K words / 3 chapters is already written using this Approach. The overall story is of 7 chapters and the rest 4 chapters are currently being written (High Level plot, chapters are already present). 

The current plotline is **what happens if the American founding fathers come back to life in 21st century**. [Story till now](https://github.com/desik1998/NovelWithLLMs/blob/main/Novel.md). I've reviewed this with many people who are avid novel readers and most have agreed that it didn't feel AI generated, interesting (subjective but most have agreed on this), grounds heavily on details. If there are any political biases, they might be of my own and request readers to avoid over emphasizing on these and in fact I wanted to keep the story less controversial.

## Approach to create long story:
Claude 3 currently allows input context length of 150K words and can output 3K words at once. A typical novel has a total of 60K-100K words. Considering the 3K word limit, it isn't possible to generate a novel in one single take. So the intuition here is that let the LLM **generate 1 event at a time and once the event is generated, add it to the existing story and continously repeat this process**. Although theoretically this approach might seem to work, just doing this leads to LLM moving quickly from one event to another, not being very grounded in details, llm not generating event which is a continuation of the current story etc. 

To address this, the following steps are taken:
### 1. Initially fix on the high story:


### 2. Do the event based generation:
Now start with the generation of 1 event at a time like described above. To make sure that the event is grounded in details, a little prompting is reqd telling the LLM to avoid moving too fast into the event and ground to details, avoid generating same events as past etc. [Prompt used till now](). Even after this, the output generated by LLM might not be very compelling so to a good output, generate the output multiple times. And in general generating 5-10 outputs, results in a good possible result. And it's better to do this by varying temperatures. In case of current story, the temperature b/w 0.4-0.8 worked well. Additionally, the rationale behind generating multiple outputs is, given LLMs generate different output everytime, the chances of getting good output when prompted multiple times increases. 

**The current approach, doesn't require any code** and long stories can be generated directly using the **Claude Playground or Amazon Bedrock Playground** (Claude is hosted). Claude Playground has the best Claude Model Opus which Bedrock currently lacks but given this Model is 10X costly, avoided it and went with the 2nd Best Sonnet Model

## Questions:
1. Why wasn't Gpt4 used to create this story?
    * When asked Gpt4 to generate the next event in the story, there was no coherence in the next event generated with the existing story. Maybe with more prompt engineering, this might be solved but Claude 3 was giving better output without much effort so went with it. Infact, Claude 3 Sonnet (the 2nd best model from Claude) is doing much better when compared to Gpt4. 
2. How much cost did it take to do this? -> $50-100

## Further Improvements:
1. Explore ways to avoid long input contexts. This can further reduce the cost considering most of the cost is going into this step. Possible Solutions:
   * Give gists of the events happened in the story till now instead of whole story as an input to the LLM. References: [1](https://deepmind.google/research/publications/74917/), [2](https://arxiv.org/html/2310.00785v3)
2. Avoid the human loop as part of the choosing the best event generated. Currently it takes a lot of human time when choosing the best event generated. Due to this, the time to generate a story can take from few weeks to few months (1-2 months). If this step is automated atleast to some degree, the time to write the long story will further decrease. Possible Solutions:
   * Use an LLM to determine what are the best events or top 2-3 events generated. This can be done based on multiple factors such as whether the event is a continuation, the event is not repeating itself. And based on these factors, LLM can rate the top responses
   * Train a reward model (With or without LLM) for determining which generated event is better. [LLM as Reward model](https://arxiv.org/html/2401.10020v1)
3. The current approach generates only 1 story. Instead generate a Tree of possible stories for a given plot. For example, multiple generations for an event can be good, in this case, select all of them and create different stories.

